from langgraph.types import Command ,interrupt 
from src.tools.search_with_image import TavilySearchWithImages
from src.llms.llm import get_llm
from langgraph.prebuilt import create_react_agent
from langgraph.graph import MessagesState,StateGraph , START , END
from typing import  List, Optional, Union
from typing_extensions import Annotated
import operator
from langchain_core.messages import HumanMessage
from langchain.tools import tool
from datetime import datetime
from src.prompts.template import render_prompt_template
from src.graph.type import Plan
import json
import logging
from langgraph.prebuilt import create_react_agent
logger = logging.getLogger(__name__)
# config = {"thread_id"}
class State(MessagesState):
    # ========== ç”¨æˆ·è¾“å…¥ ==========
    research_topic: str = ""                         # ç ”ç©¶ä¸»é¢˜ / ç”¨æˆ·é—®é¢˜
    locale: str = "zh-CN"                                # è¾“å‡ºè¯­è¨€ (en-US, zh-CN ç­‰)
    current_plan: Optional[Plan] = None                         # ç ”ç©¶è®¡åˆ’
    
    # ========== æœç´¢é˜¶æ®µ ==========
    search_queries: Annotated[List[str], operator.add] = []  # å·²ç”Ÿæˆçš„æŸ¥è¯¢
    web_results: Annotated[List[dict], operator.add] = []     # åŸå§‹æœç´¢ç»“æœ (title, url, snippet)
    sources: Annotated[List[dict], operator.add] = []         # è¿‡æ»¤/æç‚¼åçš„èµ„æ–™
    
    # ========== æ¨ç† & è®¡åˆ’ ==========
    observations: Annotated[List[str], operator.add] = []   # æ¯è½®ç ”ç©¶çš„å‘ç°/ç»“è®º
    plan_iterations: int = 0                                # è¿­ä»£æ¬¡æ•°
    auto_accept_plan: bool = False                          # æ˜¯å¦è‡ªåŠ¨æ¥å—ç”Ÿæˆçš„è®¡åˆ’
    
    # ========== èƒŒæ™¯è°ƒæŸ¥ ==========
    enable_background_investigation: bool = False        # æ˜¯å¦å¼€å¯å¹¶è¡ŒèƒŒæ™¯è°ƒæŸ¥
    background_info: Optional[str] = None                # èƒŒæ™¯ä¿¡æ¯ï¼ˆç»´åŸºç™¾ç§‘ç­‰ï¼‰
    
    # ========== ç ”ç©¶æ‰§è¡Œé˜¶æ®µ ==========
    research_summary: Optional[str] = None              # ç ”ç©¶æ€»ç»“æŠ¥å‘Š
    step_results: Annotated[List[dict], operator.add] = []     # æ¯ä¸ªç ”ç©¶æ­¥éª¤çš„è¯¦ç»†ç»“æœ
    
    # ========== æ§åˆ¶å‚æ•° ==========
    research_loop_count: int = 0                        # å½“å‰ç ”ç©¶å¾ªç¯æ•°
    max_research_loops: int = 3                                    # æœ€å¤§å¾ªç¯æ•°

llm = get_llm()
@tool 
def handoff_to_planner(
    research_topic: Annotated[str, "äº¤ç»™plannerå¤„ç†çš„ç ”ç©¶ä¸»é¢˜/ç”¨æˆ·é—®é¢˜"],
    locale: Annotated[str, "ç”¨æˆ·è¾“å…¥è¯­è¨€ (en-US, zh-CN ç­‰)"],
):
    """
    äº¤ç»™plannerå¤„ç†ç ”ç©¶ä¸»é¢˜
    """
    # è¿™ä¸ªå·¥å…·ä¸éœ€è¦è¿”å›å€¼ï¼Œåªè¦è¢«è°ƒç”¨å°±è¡¨ç¤ºéœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µ
    pass 

def coordinate_node(state: State) -> Command:
    """
    åè°ƒï¼Œå¤„ç†ç®€å•çš„ç”¨æˆ·æŸ¥è¯¢ï¼Œå¯¹äºç ”ç©¶ã€è®¡åˆ’ã€å¤æ‚çš„ä»»åŠ¡äº¤ç»™plannerå¤„ç†
    è½»é—®é¢˜ â†’ ç›´æ¥å›ç­”
    é‡é—®é¢˜ â†’ äº¤ç»™ planner
    """
    # è·å–ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜
    research_topic = state.get("research_topic", "")
    locale = state.get("locale", "zh-CN")
    
    # å¦‚æœæ²¡æœ‰ç ”ç©¶ä¸»é¢˜ï¼Œä½¿ç”¨ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºè¾“å…¥
    if not research_topic:
        # ä»æ¶ˆæ¯å†å²ä¸­è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        messages = state["messages"][-1]
        research_topic = messages.content

    # å‡†å¤‡åè°ƒå™¨æç¤ºæ¨¡æ¿
    template_vars = {
        "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        # "research_topic": research_topic,
    }
    
    # ä½¿ç”¨åè°ƒå™¨æç¤ºæ¨¡æ¿
    system_prompt = render_prompt_template("coordinate", **template_vars)
    
    llm_with_tools = llm.bind_tools([handoff_to_planner])
    
    # è¿è¡Œåè°ƒå™¨ä»£ç†
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": research_topic}
    ]
    
    result = llm_with_tools.invoke(messages)
    goto = "__end__"
    # æ£€æŸ¥å·¥å…·è°ƒç”¨
    tool_calls = []
    n = len(result.tool_calls)
    print(result.tool_calls)
    print(n)

    if n > 0:
        tool_calls.extend(result.tool_calls)
        goto = "generate_plan"
    # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†handoff_to_plannerå·¥å…·
    handoff_calls = [call for call in tool_calls if call["name"] == "handoff_to_planner"]
    
    if handoff_calls:
        # æå–å·¥å…·è°ƒç”¨çš„å‚æ•°å¹¶å¡«å……åˆ°stateä¸­
        tool_call = handoff_calls[-1]  # ä½¿ç”¨æœ€åä¸€æ¬¡è°ƒç”¨çš„å‚æ•°
        args = tool_call["args"] if "args" in tool_call else {}
    
        research_topic = args.get("research_topic", research_topic)
        locale = args.get("locale", locale)
        
        # éœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€
        return Command(
            update={
                "research_topic": research_topic,
                "locale": locale,
            },
            goto= goto,
            
        )
    else:
        # ç›´æ¥å›ç­”ç®€å•é—®é¢˜ï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯
        return Command(
            update={
                "messages": [{"role": "assistant", "content": result.content}]
            },
            goto= goto,
        )
    

def generate_plan(state: State) -> dict:
    """
    ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼šç”Ÿæˆç ”ç©¶è®¡åˆ’
    ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿè·å–ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶é€šè¿‡Jinja2æ¨¡æ¿æ›¿æ¢å˜é‡
    """
    # å‡†å¤‡æ¨¡æ¿å˜é‡
    template_vars = {
        "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "research_topic": state.get("research_topic", ""),
        "locale": state.get("locale", "zh-CN")
    }
    
    # ä½¿ç”¨å°è£…çš„render_prompt_templateæ–¹æ³•æ¸²æŸ“æ¨¡æ¿
    system_prompt = render_prompt_template("planner", **template_vars)
    
    # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{state.get('research_topic', '')}"}
    ]
    
    # è°ƒç”¨LLMç”Ÿæˆå“åº”
    struct_llm = llm.with_structured_output(Plan)
    response = struct_llm.invoke(messages)
    response_str = response.model_dump_json(indent=4)
    # è¿”å›å­—å…¸ç”¨äºæ›´æ–°çŠ¶æ€
    return {
        "current_plan": response,
        "plan_iterations": state.get("plan_iterations", 0) + 1,
        "messages": [{"role": "assistant", "content": response_str}]
    }

def human_back_node(state: State) -> Command:
    """
    ç”¨æˆ·æ˜¯å¦ç¡®è®¤planner ç”Ÿæˆçš„ç ”ç©¶è®¡åˆ’ï¼Œå¦‚æœç”¨æˆ·æœ‰è¡¥å……è¾“å…¥ï¼Œ
    åˆ™æ›´æ–°ç ”ç©¶è®¡åˆ’
    """

    feedback = interrupt("Please Review the Plan.")
    logger.info(f"ç”¨æˆ·åé¦ˆ: {feedback}")
    if feedback["response"] and str(feedback["response"]).lower() in ["ç¡®è®¤", "ok", "å¥½çš„", "åŒæ„", "accept"]:
        logger.info(feedback["response"])
        goto = "research_node"
    else:
        goto = "generate_plan"
        logger.info(feedback["response"])
        return Command(
            update={
            "messages": HumanMessage(content=feedback["response"],name = "feedback"),
            },
            goto = goto,
        )
    return Command(
        goto = goto,
    )

def research_node(state: State) -> Command:
    """
    ç ”ç©¶èŠ‚ç‚¹ï¼šæ ¹æ®è®¡åˆ’è¿›è¡Œç ”ç©¶
    éå†æ¯ä¸ªç ”ç©¶æ­¥éª¤ï¼Œä½¿ç”¨React Agentè¿›è¡Œè¿­ä»£ç ”ç©¶ï¼Œ
    å¹¶æ•´åˆæ¯æ¬¡çš„è¾“å…¥è¾“å‡ºï¼Œå½¢æˆå®Œæ•´çš„ç ”ç©¶ç»“æœ
    """
    tools = [TavilySearchWithImages()]
    
    # è·å–å½“å‰çŠ¶æ€ä¸­çš„æ¶ˆæ¯å†å²
    messages = state.get("messages", [])
    
    # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„ç ”ç©¶ç»“æœ
    step_results = []
    all_research_messages = []
    
    # éå†ç ”ç©¶è®¡åˆ’çš„æ¯ä¸ªæ­¥éª¤
    for i, step in enumerate(state["current_plan"].steps[:2]):
        logger.info(f"æ‰§è¡Œç ”ç©¶æ­¥éª¤ {i+1}: {step.title}")
        
        # åˆ›å»ºReact Agent
        agent = create_react_agent(
            model=llm,
            tools=tools,
        )
        
        # æ„å»ºå½“å‰æ­¥éª¤çš„æŸ¥è¯¢æ¶ˆæ¯
        step_query = f"""
        ç ”ç©¶æ­¥éª¤ {i+1}: {step.title}
        
        æè¿°: {step.description}
        
        è¯·åŸºäºå‰é¢çš„ç ”ç©¶ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œå¹¶æä¾›è¯¦ç»†çš„å‘ç°ã€‚
        """
        
        # å°†æ­¥éª¤æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
        current_messages = messages.copy()
        current_messages.append({"role": "user", "content": step_query})
        
        try:
            # æ‰§è¡ŒAgentç ”ç©¶
            result = agent.invoke({"messages": current_messages})
            
            if result and result.get("messages"):
                # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯ï¼ˆç ”ç©¶ç»“æœï¼‰
                last_msg = result["messages"][-1]
                
                # è®°å½•æ­¥éª¤ç»“æœ
                step_result = {
                    "step_index": i,
                    "step_title": step.title,
                    "step_description": step.description,
                    "research_result": last_msg.content,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                step_results.append(step_result)
                
                # å°†AIå“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„ä¸Šä¸‹æ–‡
                messages.append({"role": "user", "content": step_query})
                messages.append({"role": "assistant", "content": last_msg.content})
                all_research_messages.append(last_msg.content)
                
                logging.info(f"æ­¥éª¤ {i+1} å®Œæˆç ”ç©¶ç»“æœ")
                
        except Exception as e:
            print(f"æ‰§è¡Œæ­¥éª¤ {i+1} æ—¶å‡ºé”™: {e}")
            # è®°å½•é”™è¯¯ä¿¡æ¯
            error_result = {
                "step_index": i,
                "step_title": step.title,
                "step_description": step.description,
                "research_result": f"ç ”ç©¶æ­¥éª¤æ‰§è¡Œå¤±è´¥: {str(e)}",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "error": True
            }
            step_results.append(error_result)
    
    # æ•´åˆæ‰€æœ‰ç ”ç©¶ç»“æœ
    research_summary = f"""
# ç ”ç©¶æŠ¥å‘Š: {state['current_plan'].title}

## ç ”ç©¶èƒŒæ™¯
{state['current_plan'].thought}

## è¯¦ç»†ç ”ç©¶æ­¥éª¤ä¸å‘ç°

"""
    
    for result in step_results:
        research_summary += f"""
### æ­¥éª¤ {result['step_index'] + 1}: {result['step_title']}

**æè¿°**: {result['step_description']}

**ç ”ç©¶å‘ç°**:
{result['research_result']}

---

"""
    
    research_summary += f"""
## ç ”ç©¶æ€»ç»“

æœ¬æ¬¡ç ”ç©¶å…±æ‰§è¡Œäº† {len(step_results)} ä¸ªæ­¥éª¤ï¼Œå½¢æˆäº†å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šã€‚
ç ”ç©¶å®Œæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return Command(
        update={
            "messages": messages,  # æ›´æ–°å®Œæ•´çš„æ¶ˆæ¯å†å²
            "observations": state.get("observations", []) + all_research_messages,  # æ·»åŠ ç ”ç©¶å‘ç°
            "research_summary": research_summary,  # æ·»åŠ ç ”ç©¶æ€»ç»“
            "step_results": step_results,  # æ·»åŠ æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ç»“æœ
            "research_loop_count": state.get("research_loop_count", 0) + 1  # å¢åŠ ç ”ç©¶å¾ªç¯è®¡æ•°
        },
        goto="__end__"  # ç ”ç©¶å®Œæˆåç»“æŸæµç¨‹
    )
        
# æ„å»ºå®Œæ•´çš„ç ”ç©¶æµç¨‹å›¾
graph_build = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph_build.add_node("coordinate", coordinate_node)
graph_build.add_node("generate_plan", generate_plan)
graph_build.add_node("human_feedback", human_back_node)
graph_build.add_node("research_node", research_node)

# å®šä¹‰æµç¨‹
graph_build.add_edge(START, "coordinate")
graph_build.add_edge("coordinate", "generate_plan")
graph_build.add_edge("generate_plan", "human_feedback")
graph_build.add_edge("human_feedback", "research_node")
graph_build.add_edge("research_node", END)


# é…ç½®æ£€æŸ¥ç‚¹
from langgraph.checkpoint.memory import InMemorySaver
memory = InMemorySaver()

# ç¼–è¯‘å›¾æ—¶é…ç½®æ£€æŸ¥ç‚¹
graph = graph_build.compile(checkpointer=memory)

def test_research_flow(research_topic: str = "2025å¹´token2049å¤§ä¼šæœ‰å“ªäº›é‡è¦è®®é¢˜å’Œæ¼”è®²ï¼Ÿ", locale: str = "zh-CN"):
    """
    æµ‹è¯•å®Œæ•´çš„ç ”ç©¶æµç¨‹
    
    Args:
        research_topic: ç ”ç©¶ä¸»é¢˜
        locale: è¯­è¨€è®¾ç½®
    
    Returns:
        dict: åŒ…å«æµ‹è¯•ç»“æœå’Œæœ€ç»ˆçŠ¶æ€çš„å­—å…¸
    """
    print("=== ç ”ç©¶æµç¨‹æµ‹è¯•å¼€å§‹ ===")
    print(f"ç ”ç©¶ä¸»é¢˜: {research_topic}")
    print(f"è¯­è¨€è®¾ç½®: {locale}")
    
    config = {
        "configurable": {
            "thread_id": "test_research"
        }
    }
    
    # ä¿å­˜æµç¨‹å›¾
    try:
        with open("graph.md", "wb") as f:
            f.write(graph.get_graph().draw_mermaid().encode("utf-8"))
            print("âœ… æµç¨‹å›¾å·²ä¿å­˜ä¸º graph.md")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æµç¨‹å›¾å¤±è´¥: {e}")
    
    # åˆ›å»ºæµ‹è¯•çŠ¶æ€
    test_state = State(
        messages=[HumanMessage(content=research_topic, name="ç”¨æˆ·è¾“å…¥")],
        research_topic=research_topic,
        locale=locale
    )
    
    results = {
        "research_topic": research_topic,
        "locale": locale,
        "stages": [],
        "final_state": None,
        "success": False,
        "error": None
    }
    
    try:
        # é˜¶æ®µ1: åè°ƒå™¨
        print("\nğŸ“‹ é˜¶æ®µ1: åè°ƒå™¨åˆ†æ...")
        for result in graph.stream(test_state, config):
            stage_name = list(result.keys())[0]
            results["stages"].append(stage_name)
            print(f"   âœ… {stage_name}")
        
        # é˜¶æ®µ2: è®¡åˆ’ç”Ÿæˆ
        print("\nğŸ“ é˜¶æ®µ2: ç”Ÿæˆç ”ç©¶è®¡åˆ’...")
        for result in graph.stream(test_state, config):
            if "generate_plan" in result:
                plan = result["generate_plan"]["current_plan"]
                print(f"   âœ… è®¡åˆ’æ ‡é¢˜: {plan.title}")
                print(f"   âœ… è®¡åˆ’åŒ…å« {len(plan.steps)} ä¸ªç ”ç©¶æ­¥éª¤")
                results["plan"] = {
                    "title": plan.title,
                    "thought": plan.thought,
                    "steps_count": len(plan.steps),
                    "steps": [{"title": step.title, "description": step.description} for step in plan.steps]
                }
        
        # é˜¶æ®µ3: ç”¨æˆ·ç¡®è®¤ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\nğŸ‘¤ é˜¶æ®µ3: ç”¨æˆ·ç¡®è®¤ç ”ç©¶è®¡åˆ’...")
        from langgraph.types import Command
        for chunk in graph.stream(Command(resume={"response": "ç¡®è®¤"}), config):
            if "human_feedback" in chunk:
                print("   âœ… ç”¨æˆ·å·²ç¡®è®¤è®¡åˆ’")
        
        # é˜¶æ®µ4: ç ”ç©¶æ‰§è¡Œ
        print("\nğŸ” é˜¶æ®µ4: æ‰§è¡Œç ”ç©¶è®¡åˆ’...")
        final_state = None
        research_results = graph.invoke(Command(resume={"response": "ç¡®è®¤"}), config)
        results["final_state"] = research_results
        results["success"] = True
        return results
        # for chunk in graph.stream(Command(resume={"response": "ç¡®è®¤"}), config):
        #     if "research_node" in chunk:
        #         final_state = chunk["research_node"]
        #         results["final_state"] = final_state
                
        #         print(f"   âœ… ç ”ç©¶æ‰§è¡Œå®Œæˆ")
        #         print(f"   ğŸ“Š ç ”ç©¶å¾ªç¯æ•°: {final_state.get('research_loop_count', 0)}")
        #         print(f"   ğŸ“Š ç ”ç©¶å‘ç°æ•°é‡: {len(final_state.get('observations', []))}")
        #         print(f"   ğŸ“Š æ­¥éª¤ç»“æœæ•°é‡: {len(final_state.get('step_results', []))}")
                
        #         if "research_summary" in final_state:
        #             summary_length = len(final_state["research_summary"])
        #             print(f"   ğŸ“Š ç ”ç©¶æ€»ç»“é•¿åº¦: {summary_length} å­—ç¬¦")
                
        #         results["success"] = True
        #         break
        
        # print("\nğŸ‰ ç ”ç©¶æµç¨‹æµ‹è¯•å®Œæˆï¼")
        # return results
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        results["error"] = str(e)
        import traceback
        traceback.print_exc()
        return results

def run_specific_test():
    """è¿è¡Œç‰¹å®šçš„æµ‹è¯•ç”¨ä¾‹"""
    print("=== è¿è¡Œç‰¹å®šæµ‹è¯• ===")
    
    # æµ‹è¯•ç”¨ä¾‹1: ç®€å•é—®é¢˜
    print("\nğŸ§ª æµ‹è¯•ç”¨ä¾‹1: ç®€å•ç ”ç©¶é—®é¢˜")
    result1 = test_research_flow("ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼Ÿ", "zh-CN")
    
    # æµ‹è¯•ç”¨ä¾‹2: å¤æ‚é—®é¢˜
    print("\nğŸ§ª æµ‹è¯•ç”¨ä¾‹2: å¤æ‚ç ”ç©¶é—®é¢˜")
    result2 = test_research_flow("2024å¹´äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„æœ€æ–°å‘å±•å’Œåº”ç”¨æ¡ˆä¾‹", "zh-CN")
    
    # æµ‹è¯•ç”¨ä¾‹3: è‹±æ–‡é—®é¢˜
    print("\nğŸ§ª æµ‹è¯•ç”¨ä¾‹3: è‹±æ–‡ç ”ç©¶é—®é¢˜")
    result3 = test_research_flow("What are the latest developments in quantum computing?", "en-US")
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\nğŸ“ˆ æµ‹è¯•ç»“æœæ€»ç»“:")
    for i, result in enumerate([result1, result2, result3], 1):
        status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
        print(f"   æµ‹è¯•ç”¨ä¾‹{i}: {status}")
        if result["success"] and "plan" in result:
            print(f"     - è®¡åˆ’æ­¥éª¤: {result['plan']['steps_count']} ä¸ª")
            print(f"     - ç ”ç©¶å‘ç°: {len(result['final_state']['observations'])} æ¡")

# æµ‹è¯•è¿è¡Œ
if __name__ == "__main__":
    import sys
    
    # # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    # if len(sys.argv) > 1:
    #     if sys.argv[1] == "test":
    #         # è¿è¡ŒåŸºç¡€æµ‹è¯•
    #         if len(sys.argv) > 2:
    #             # è‡ªå®šä¹‰æµ‹è¯•é—®é¢˜
    #             research_topic = " ".join(sys.argv[2:])
    #             result = test_research_flow(research_topic)
    #         else:
    #             # é»˜è®¤æµ‹è¯•
    #             result = test_research_flow()
    #     elif sys.argv[1] == "full_test":
    #         # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
    #         run_specific_test()
    #     else:
    #         print("ç”¨æ³•:")
    #         print("  python node.py test [ç ”ç©¶é—®é¢˜]    - è¿è¡Œå•ä¸ªæµ‹è¯•")
    #         print("  python node.py full_test          - è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶")
    #         print("  python node.py                    - è¿è¡Œé»˜è®¤æµ‹è¯•")
    # else:
        # é»˜è®¤è¡Œä¸ºï¼šè¿è¡ŒåŸºç¡€æµ‹è¯•
    print("è¿è¡Œé»˜è®¤æµ‹è¯•...")
    result = test_research_flow()
    
    # å¦‚æœæµ‹è¯•æˆåŠŸï¼Œæ˜¾ç¤ºç ”ç©¶æ€»ç»“é¢„è§ˆ
    if result["success"] and result["final_state"] :
        print("\n=== ç ”ç©¶æ€»ç»“é¢„è§ˆ ===")
        summary = result["final_state"]
        print(summary)
    # # æµ‹è¯•2: éœ€è¦ç ”ç©¶çš„é—®é¢˜
    # print("\n--- æµ‹è¯•éœ€è¦ç ”ç©¶çš„é—®é¢˜ ---")
    # test_state2 = State(
    #     research_topic="2025å¹´token2049å¤§ä¼šæœ‰å“ªäº›é‡è¦è®®é¢˜å’Œæ¼”è®²ï¼Ÿ",
    #     locale="zh-CN"
    # )
    # result2 = graph.invoke(test_state2)
    # print(f"åè°ƒå™¨å“åº”: {result2['messages'][-1]['content']}")
    
    # # å¦‚æœæœ‰ç ”ç©¶è®¡åˆ’ï¼Œç»§ç»­æµ‹è¯•ç ”ç©¶æµç¨‹
    # if result2.get("current_plan"):
    #     print(f"\nç”Ÿæˆçš„ç ”ç©¶è®¡åˆ’:")
    #     print(f"æ ‡é¢˜: {result2['current_plan'].title}")
    #     print(f"æ€è€ƒè¿‡ç¨‹: {result2['current_plan'].thought}")
    #     print("ç ”ç©¶æ­¥éª¤:")
    #     for i, step in enumerate(result2['current_plan'].steps):
    #         print(f"  {i+1}. {step.title}: {step.description}")
        
    #     # æµ‹è¯•ç ”ç©¶æ‰§è¡Œ
    #     print("\n--- æ‰§è¡Œç ”ç©¶æ­¥éª¤ ---")
    #     tools = [TavilySearchWithImages()]
    #     agent = create_react_agent(llm, tools)
        
    #     for i, step in enumerate(result2["current_plan"].steps):
    #         print(f"\næ‰§è¡Œæ­¥éª¤ {i+1}: {step.title}")
    #         try:
    #             res = agent.invoke({"messages": [{"role": "user", "content": step.description}]})
    #             if res and res.get("messages"):
    #                 last_msg = res["messages"][-1]
    #                 print(f"ç ”ç©¶ç»“æœ: {last_msg.content[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
    #         except Exception as e:
    #             print(f"æ‰§è¡Œæ­¥éª¤æ—¶å‡ºé”™: {e}")
    
    # print("\n=== æµ‹è¯•å®Œæˆ ===")


# # å·¥å…·å‡½æ•°ï¼šç”¨äºåç»­å¤„ç†
# def process_search_results(content: str) -> str:
#     """
#     å¤„ç†æœç´¢ç»“æœä¸­çš„å›¾ç‰‡é“¾æ¥ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼
#     """
#     import re
    
#     # åŒ¹é…é“¾æ¥æ¨¡å¼
#     pattern = re.compile(r'\[.*?\]\((https?://[^\s)]+)\)')
    
#     # æ›¿æ¢æˆ Markdown å›¾ç‰‡è¯­æ³•
#     def replace_with_img(match):
#         url = match.group(1)
#         return f'![]({url})'
    
#     new_content = pattern.sub(replace_with_img, content)
    
#     # å¦‚æœæ–‡æœ¬é‡Œè¿˜æœ‰è£¸ URLï¼Œä¹Ÿå¯ä»¥é¢å¤–åŒ¹é…
#     url_pattern = re.compile(r'(?<!\]\()(?<!["\'])https?://[^\s]+')
#     new_content = url_pattern.sub(lambda m: f'![]({m.group(0)})', new_content)
    
#     return new_content

# def create_research_plan(state: State) -> dict:
#     """
#     åˆ›å»ºç ”ç©¶è®¡åˆ’èŠ‚ç‚¹
#     ç”Ÿæˆç ”ç©¶è®¡åˆ’å¹¶æ›´æ–°çŠ¶æ€
#     """
#     # ç”Ÿæˆç ”ç©¶æŸ¥è¯¢
#     plan_content = generate_query(state)
    
#     # å¤„ç†æœç´¢ç»“æœä¸­çš„å›¾ç‰‡é“¾æ¥
#     processed_content = process_search_results(plan_content)
    
#     # æ›´æ–°çŠ¶æ€
#     return {
#         "current_plan": processed_content,
#         "plan_iterations": state.get("plan_iterations", 0) + 1,
#         "messages": [{"role": "assistant", "content": processed_content}]
#     }

