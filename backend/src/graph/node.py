from langgraph.types import Command ,interrupt 
from src.tools.search_with_image import TavilySearchWithImages
from src.llms.llm import get_llm
from langgraph.prebuilt import create_react_agent
from langgraph.graph import MessagesState,StateGraph , START , END
from typing import  List, Optional, Union, Tuple
from typing_extensions import Annotated
import operator
import re
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.tools import tool
from datetime import datetime
from src.prompts.template import render_prompt_template
from src.graph.type import Plan
import json
import logging
import asyncio
from langgraph.prebuilt import create_react_agent
from src.utils.content import ContextManager
logger = logging.getLogger(__name__)
# config = {"thread_id"}
class State(MessagesState):
    # ========== ç”¨æˆ·è¾“å…¥ ==========
    research_topic: str = ""                         # ç ”ç©¶ä¸»é¢˜ / ç”¨æˆ·é—®é¢˜
    locale: str = "zh-CN"                                # è¾“å‡ºè¯­è¨€ (en-US, zh-CN ç­‰)
    current_plan: Optional[Plan] = None                         # ç ”ç©¶è®¡åˆ’
    
    # ========== æœç´¢é˜¶æ®µ ==========
    search_queries: Annotated[List[str], operator.add] = []  # å·²ç”Ÿæˆçš„æŸ¥è¯¢
    web_results: Annotated[List[dict], operator.add] = []     # åŸå§‹æœç´¢ç»“æœ (title, url, snippet)
    sources: Annotated[List[dict], operator.add] = []         # è¿‡æ»¤/æç‚¼åçš„èµ„æ–™
    
    # ========== æ¨ç† & è®¡åˆ’ ==========
    observations: Annotated[List[str], operator.add] = []   # æ¯è½®ç ”ç©¶çš„å‘ç°/ç»“è®º
    plan_iterations: int = 0                                # è¿­ä»£æ¬¡æ•°
    auto_accept_plan: bool = False                          # æ˜¯å¦è‡ªåŠ¨æ¥å—ç”Ÿæˆçš„è®¡åˆ’
    
    # ========== èƒŒæ™¯è°ƒæŸ¥ ==========
    enable_background_investigation: bool = False        # æ˜¯å¦å¼€å¯å¹¶è¡ŒèƒŒæ™¯è°ƒæŸ¥
    background_info: Optional[str] = None                # èƒŒæ™¯ä¿¡æ¯ï¼ˆç»´åŸºç™¾ç§‘ç­‰ï¼‰
    
    # ========== ç ”ç©¶æ‰§è¡Œé˜¶æ®µ ==========
    research_summary: Optional[str] = None              # ç ”ç©¶æ€»ç»“æŠ¥å‘Š
    step_results: Annotated[List[dict], operator.add] = []     # æ¯ä¸ªç ”ç©¶æ­¥éª¤çš„è¯¦ç»†ç»“æœ
    
    # ========== æ§åˆ¶å‚æ•° ==========
    research_loop_count: int = 0                        # å½“å‰ç ”ç©¶å¾ªç¯æ•°
    max_research_loops: int = 3                                    # æœ€å¤§å¾ªç¯æ•°

llm = get_llm()
@tool 
def handoff_to_planner(
    research_topic: Annotated[str, "äº¤ç»™plannerå¤„ç†çš„ç ”ç©¶ä¸»é¢˜/ç”¨æˆ·é—®é¢˜"],
    locale: Annotated[str, "ç”¨æˆ·è¾“å…¥è¯­è¨€ (en-US, zh-CN ç­‰)"],
):
    """
    äº¤ç»™plannerå¤„ç†ç ”ç©¶ä¸»é¢˜
    """
    # è¿™ä¸ªå·¥å…·ä¸éœ€è¦è¿”å›å€¼ï¼Œåªè¦è¢«è°ƒç”¨å°±è¡¨ç¤ºéœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µ
    pass 

def coordinate_node(state: State) -> Command:
    """
    åè°ƒï¼Œå¤„ç†ç®€å•çš„ç”¨æˆ·æŸ¥è¯¢ï¼Œå¯¹äºç ”ç©¶ã€è®¡åˆ’ã€å¤æ‚çš„ä»»åŠ¡äº¤ç»™plannerå¤„ç†
    è½»é—®é¢˜ â†’ ç›´æ¥å›ç­”
    é‡é—®é¢˜ â†’ äº¤ç»™ planner
    """
    chunks = []
    for chunk in llm.stream(state["messages"]):
        delta = getattr(chunk, "content", None)
        if delta:
            # print(delta, end="", flush=True)
            chunks.append(delta)
    full_text = "".join(chunks)
    state["messages"].append(AIMessage(content=full_text))
    state["messages"].append(HumanMessage(content="è¯¦ç»†è¾©è®ºä¸Šé¢ä½ çš„å›ç­”"))
    # print(f"{state} A")
    # state["term"] += 1
    return Command(
        update={
            "messages": [AIMessage(content=full_text)]
        },
        goto="__end__"
    )
    # # è·å–ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜
    # research_topic = state.get("research_topic", "")
    # locale = state.get("locale", "zh-CN")
    
    # # å¦‚æœæ²¡æœ‰ç ”ç©¶ä¸»é¢˜ï¼Œä½¿ç”¨ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºè¾“å…¥
    # if not research_topic:
    #     # ä»æ¶ˆæ¯å†å²ä¸­è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    #     messages = state["messages"][-1]
    #     research_topic = messages.content

    # # å‡†å¤‡åè°ƒå™¨æç¤ºæ¨¡æ¿
    # template_vars = {
    #     "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    #     # "research_topic": research_topic,
    # }
    # # ä½¿ç”¨åè°ƒå™¨æç¤ºæ¨¡æ¿
    # system_prompt = render_prompt_template("coordinate", **template_vars)
    # llm_with_tools = llm.bind_tools([handoff_to_planner])
    
    # # è¿è¡Œåè°ƒå™¨ä»£ç†
    # system_messages = [SystemMessage(content=system_prompt)]
    # human_messages = [HumanMessage(content=research_topic)]
    # messages = system_messages + human_messages

    # # æµå¼ç´¯ç§¯å†…å®¹ä¸å·¥å…·è°ƒç”¨
    # content_chunks = []
    # tool_calls = []
    # final_chunk = None
    # for chunk in llm_with_tools.stream(messages):
    #     # ç´¯ç§¯æ–‡æœ¬
    #     delta = getattr(chunk, "content", None)
    #     if delta:
    #         content_chunks.append(delta)
    #     # ç´¯ç§¯å·¥å…·è°ƒç”¨ï¼ˆå…¼å®¹ä¸åŒå®ç°çš„å¢é‡ï¼‰
    #     if hasattr(chunk, "tool_calls") and chunk.tool_calls:
    #         try:
    #             # å¯èƒ½æ˜¯åˆ—è¡¨å¢é‡ï¼Œç›´æ¥å¹¶å…¥
    #             tool_calls.extend(chunk.tool_calls)
    #         except Exception:
    #             pass
    #     final_chunk = chunk

    # # è‹¥å¢é‡æœªæºå¸¦å·¥å…·ï¼Œå°è¯•ä»æœ€ç»ˆå—è¯»å–
    # if not tool_calls and final_chunk is not None and hasattr(final_chunk, "tool_calls"):
    #     try:
    #         tool_calls = list(getattr(final_chunk, "tool_calls", []) or [])
    #     except Exception:
    #         tool_calls = []

    # full_content = "".join(content_chunks)
    # goto = "__end__"

    # # æ— å·¥å…·è°ƒç”¨ï¼šç›´æ¥å›ç­”ç®€å•é—®é¢˜
    # if not tool_calls:
    #     print(f"goto: {goto}")
    #     ai_message = AIMessage(content=full_content)
    #     return Command(
    #         update={
    #             "messages": [ai_message]
    #         },
    #         goto= goto,
    #     )

    # # æœ‰å·¥å…·è°ƒç”¨ï¼šè¿›å…¥ç”Ÿæˆè®¡åˆ’
    # goto = "generate_plan"    
    # # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†handoff_to_plannerå·¥å…·
    # # å…¼å®¹ä¸åŒç»“æ„ï¼ˆå¯¹è±¡/å­—å…¸ï¼‰
    # def _normalize_call(call):
    #     if isinstance(call, dict):
    #         return call
    #     name = getattr(call, "name", None)
    #     args = getattr(call, "args", None) or getattr(call, "arguments", None)
    #     try:
    #         if hasattr(args, "model_dump"):
    #             args = args.model_dump()
    #         elif hasattr(args, "dict"):
    #             args = args.dict()
    #     except Exception:
    #         pass
    #     return {"name": name, "args": args}

    # norm_calls = [_normalize_call(c) for c in tool_calls]
    # handoff_calls = [call for call in norm_calls if call.get("name") == "handoff_to_planner"]
    # print(f"handoff_calls: {handoff_calls}")
    # if handoff_calls:
    #     # æå–å·¥å…·è°ƒç”¨çš„å‚æ•°å¹¶å¡«å……åˆ°stateä¸­
    #     tool_call = handoff_calls[-1]  # ä½¿ç”¨æœ€åä¸€æ¬¡è°ƒç”¨çš„å‚æ•°
    #     args = tool_call.get("args", {}) or {}

    #     research_topic = args.get("research_topic", research_topic)
    #     locale = args.get("locale", locale)
        
    #     # éœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€
    #     return Command(
    #         update={
    #             "research_topic": research_topic,
    #             "locale": locale,
    #         },
    #         goto= goto,
            
    #     )
    


def generate_plan(state: State) -> dict:
    """
    ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼šç”Ÿæˆç ”ç©¶è®¡åˆ’
    ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿè·å–ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶é€šè¿‡Jinja2æ¨¡æ¿æ›¿æ¢å˜é‡
    """
    # å‡†å¤‡æ¨¡æ¿å˜é‡
    template_vars = {
        "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "research_topic": state.get("research_topic", ""),
        "locale": state.get("locale", "zh-CN")
    }
    
    # ä½¿ç”¨å°è£…çš„render_prompt_templateæ–¹æ³•æ¸²æŸ“æ¨¡æ¿
    system_prompt = render_prompt_template("planner", **template_vars)
    
    # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{state.get('research_topic', '')}"}
    ]
    
    # è°ƒç”¨LLMç”Ÿæˆå“åº”
    struct_llm = llm.with_structured_output(Plan)
    response = struct_llm.invoke(messages)
    response_str = response.model_dump_json(indent=4)
    ai_message = AIMessage(content=response_str)
    # è¿”å›å­—å…¸ç”¨äºæ›´æ–°çŠ¶æ€
    return {
        "current_plan": response,
        "plan_iterations": state.get("plan_iterations", 0) + 1,
        "messages": [ai_message]
    }

def human_back_node(state: State) -> Command:
    """
    ç”¨æˆ·æ˜¯å¦ç¡®è®¤planner ç”Ÿæˆçš„ç ”ç©¶è®¡åˆ’ï¼Œå¦‚æœç”¨æˆ·æœ‰è¡¥å……è¾“å…¥ï¼Œ
    åˆ™æ›´æ–°ç ”ç©¶è®¡åˆ’
    """

    feedback = interrupt("Please Review the Plan.")
    logger.info(f"ç”¨æˆ·åé¦ˆ: {feedback}")
    if feedback["user_confirm"] == "confirm":
        logger.info(f"ç”¨æˆ·ç¡®è®¤è®¡åˆ’ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶")
        goto = "research_node"
    else:
        goto = "generate_plan"
        logger.info(f"ç”¨æˆ·è¦æ±‚ä¿®æ”¹è®¡åˆ’ï¼Œé‡æ–°ç”Ÿæˆ")
        return Command(
            update={
            "messages": HumanMessage(content=feedback["message"],name = "feedback"),
            },
            goto = goto,
        )
    return Command(
        goto = goto,
    )
def critic_node(state: State) -> Command:
    """
    æ‰¹åˆ¤èŠ‚ç‚¹ï¼š
    å¯¹ç ”ç©¶ç»“æœè¿›è¡Œæ‰¹åˆ¤ï¼Œåˆ¤æ–­æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
    """
    pass
# langgraph å›æº¯æœºåˆ¶ï¼š
# 1. å½“æ‰¹åˆ¤èŠ‚ç‚¹åˆ¤æ–­ç ”ç©¶ç»“æœä¸ç¬¦åˆé¢„æœŸæ—¶ï¼Œ
#    ä¼šè§¦å‘å›æº¯æœºåˆ¶ï¼Œå°†å½“å‰ç ”ç©¶èŠ‚ç‚¹çš„çŠ¶æ€å›æ»šåˆ°ä¸Šä¸€ä¸ªçŠ¶æ€ã€‚
# 2. å›æº¯æœºåˆ¶ä¼šæ ¹æ®å½“å‰çŠ¶æ€ä¸­çš„ messages åˆ—è¡¨ï¼Œ
#    æ‰¾åˆ°æœ€è¿‘çš„ä¸€æ¬¡ç ”ç©¶èŠ‚ç‚¹çš„çŠ¶æ€ï¼Œ
#    å¹¶å°†å…¶ä½œä¸ºå½“å‰çŠ¶æ€ã€‚


def _extract_links_and_images_from_md(md: str) -> Tuple[List[str], List[str]]:
    """
    ä» Markdown ä¸­æå–é“¾æ¥å’Œå›¾ç‰‡ URLï¼ˆå»é‡ï¼ŒæŒ‰å‡ºç°é¡ºåºï¼‰ã€‚
    - é“¾æ¥æ ¼å¼ [text](http...)
    - å›¾ç‰‡æ ¼å¼ ![alt](http...)
    - ä¹Ÿå°è¯•åŒ¹é…è£¸ URL
    """
    if not md:
        return [], []
    links = []
    images = []
    # å›¾ç‰‡ä¼˜å…ˆï¼ˆå®ƒä¹Ÿæ˜¯é“¾æ¥å½¢å¼ï¼‰
    for m in re.finditer(r'!\[[^\]]*\]\((https?://[^\s)]+)\)', md):
        url = m.group(1).strip()
        if url not in images:
            images.append(url)
        if url not in links:
            links.append(url)
    # æ™®é€šé“¾æ¥
    for m in re.finditer(r'\[[^\]]*\]\((https?://[^\s)]+)\)', md):
        url = m.group(1).strip()
        if url not in links:
            links.append(url)
    # è£¸ urlï¼ˆé¿å…é‡å¤ï¼‰
    for m in re.finditer(r'(https?://[^\s\)\]]+)', md):
        url = m.group(1).strip().rstrip(').,')
        if url not in links:
            links.append(url)
    return links, images


async def _async_add_summary_and_references(report_md: str) -> str:
    """
    å¼‚æ­¥ç‰ˆæœ¬ï¼šä¸ºç ”ç©¶æŠ¥å‘Šæ·»åŠ æ€»ç»“å’Œå¼•ç”¨ã€‚
    """
    # ä½¿ç”¨llm æ€»ç»“
    prompt = render_prompt_template("summary")
    
    # å¼‚æ­¥è°ƒç”¨llm æ€»ç»“æŠ¥å‘Š
    sys_msg = SystemMessage(content=prompt)
    human_msg = HumanMessage(content=report_md)
    # æµå¼ç´¯ç§¯æ€»ç»“å†…å®¹
    content_chunks = []
    async for chunk in llm.astream([sys_msg, human_msg]):
        delta = getattr(chunk, "content", None)
        if delta:
            content_chunks.append(delta)
    summary_md = "".join(content_chunks)
    
    # æå–æŠ¥å‘Šä¸­çš„é“¾æ¥å’Œå›¾ç‰‡
    links, images = _extract_links_and_images_from_md(report_md)
    # ç”Ÿæˆå¼•ç”¨åˆ—è¡¨
    references = []
    for i, url in enumerate(links + images):
        references.append(f"[{i+1}] {url}")
    # åˆå¹¶å¼•ç”¨
    references_md = "## å¼•ç”¨åˆ—è¡¨:\n\n" + "\n".join(references)


    # åˆå¹¶åˆ°æŠ¥å‘Š
    report_md = report_md + "\n\n" + summary_md + "\n\n" + references_md
    return report_md


async def async_research_node(state: State) -> Command:
    """
    å¼‚æ­¥å¹¶è¡Œç‰ˆæœ¬ï¼š
    - research_agent ä¸ report_agent å¹¶è¡Œ
    - report-agent ä¸²è¡Œä¾èµ–ï¼šå¿…é¡»ç­‰å¾…å‰ä¸€æ­¥çš„reportç»“æœ
    step1.research  â”€â”€â”€â”€â”€â”€â”
                      â”‚  (ç”Ÿæˆç»“æœä¼ å…¥reporté˜Ÿåˆ—)
                      â–¼
             step1.report â”€â”€â”€â”€â”€â”€â”
                      |          â–¼
step2.research  â”€â”€â”€â”€â”€â”€â”        åˆå¹¶report
                      â”‚
                      â–¼
             step2.report â”€â”€â”€â”€â”€â”€â”
                                â–¼
                          æœ€ç»ˆæ±‡æ€»

    """
    tools = [TavilySearchWithImages()]
    messages = state.get("messages", []) or []
    research_context_manager = ContextManager(llm, max_tokens=32768)
    report_context_manager = ContextManager(llm, max_tokens=163840)

    existing_report = state.get("research_summary", "")
    if not existing_report:
        existing_report = f"# ç ”ç©¶æŠ¥å‘Š: {state['current_plan'].title}\n\n" \
                          f"## èƒŒæ™¯ä¸ç ”ç©¶åŠ¨æœº\n{state['current_plan'].thought}\n\n"
    report_md = existing_report

    RESEARCH_AGENT_SYSTEM = render_prompt_template("research")
    REPORT_AGENT_SYSTEM = render_prompt_template("report")

    step_results = []
    all_research_messages = []

    # é˜Ÿåˆ—ä¸åŒæ­¥é”
    report_queue = asyncio.Queue()
    report_lock = asyncio.Lock()  # ç¡®ä¿reportä¸²è¡Œæ‰§è¡Œ

    async def research_worker(step_index, step):
        """æ‰§è¡Œå•ä¸ªresearch stepå¹¶æ¨å…¥report_queue"""
        nonlocal messages
        try:
            logger.info(f"[research] æ‰§è¡Œæ­¥éª¤ {step_index}: {step.title}")
            step_user_prompt = f"""
å½“å‰ç ”ç©¶æ­¥éª¤ {step_index}: {step.title}
æè¿°: {step.description}

è¦æ±‚ï¼š
- è¾“å‡ºä¸º Markdownï¼ˆåŒ…å«æ ‡é¢˜ã€åˆ†æã€ç»“è®ºï¼‰ã€‚
- æ­£æ–‡ä¸­ä½¿ç”¨ Markdown é“¾æ¥æ ¼å¼ `[æè¿°æ–‡å­—](URL)` æ ‡æ³¨å¼•ç”¨ã€‚
- **ä¿æŒ Markdown æ ¼å¼**: å›¾ç‰‡ä½¿ç”¨æ ‡å‡† Markdown è¯­æ³• `![æè¿°æ–‡å­—](å›¾ç‰‡URL)`ã€‚
"""
            current_messages = messages + [
                SystemMessage(content=RESEARCH_AGENT_SYSTEM),
                HumanMessage(content=step_user_prompt)
            ]
            current_messages = research_context_manager.compress_messages(current_messages)

            research_agent = create_react_agent(model=llm, tools=tools)
            result = await research_agent.ainvoke({"messages": current_messages})
            step_md = result["messages"][-1].content if isinstance(result["messages"][-1], AIMessage) else str(result["messages"][-1])

            links, images = _extract_links_and_images_from_md(step_md)

            await report_queue.put({
                "step_index": step_index,
                "title": step.title,
                "description": step.description,
                "step_md": step_md,
                "sources": links,
                "images": images
            })

            logger.info(f"[research] æ­¥éª¤ {step_index} å®Œæˆï¼Œå·²æ¨å…¥reporté˜Ÿåˆ—")

        except Exception as e:
            logger.exception(f"[research] æ­¥éª¤ {step_index} å‡ºé”™: {e}")
            step_results.append({
                "step_index": step_index,
                "title": step.title,
                "description": step.description,
                "result_markdown": f"ç ”ç©¶å¤±è´¥: {str(e)}",
                "sources": [],
                "images": [],
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "error": True
            })

    async def report_worker():
        """æŒ‰é˜Ÿåˆ—é¡ºåºä¸²è¡Œæ•´åˆæŠ¥å‘Š"""
        nonlocal report_md
        while True:
            item = await report_queue.get()
            if item is None:
                break  # ç»“æŸä¿¡å·
            async with report_lock:
                step_index = item["step_index"]
                step_md = item["step_md"]
                logger.info(f"[report] åˆå¹¶æ­¥éª¤ {step_index}")

                report_user_prompt = f"""
previous_report: '''{report_md}'''
latest_step: '''{step_md}'''

è¯·è¾“å‡ºå¢é‡å†…å®¹ï¼šä»…åŒ…å«æœ€æ–° step çš„ Markdownã€‚
"""
                report_messages = messages + [
                    SystemMessage(content=REPORT_AGENT_SYSTEM),
                    HumanMessage(content=report_user_prompt)
                ]
                report_messages = report_context_manager.compress_messages(report_messages)

                # æµå¼ç”Ÿæˆå¢é‡å†…å®¹
                inc_chunks = []
                async for rchunk in llm.astream(report_messages):
                    delta = getattr(rchunk, "content", None)
                    if delta:
                        inc_chunks.append(delta)
                increment_md = "".join(inc_chunks)
                print(f"[report] æ­¥éª¤ {step_index} å¢é‡å†…å®¹: {increment_md}")
                # æ£€æŸ¥ increment_md æ˜¯å¦åŒ…å« existing_report çš„æ ‡é¢˜+èƒŒæ™¯
                existing_header = f"# ç ”ç©¶æŠ¥å‘Š: {state['current_plan'].title}\n\n" \
                                f"## èƒŒæ™¯ä¸ç ”ç©¶åŠ¨æœº\n{state['current_plan'].thought}\n\n"
                if increment_md.startswith(existing_header):
                    report_md = ""  # æ¸…ç©ºå·²æœ‰ report_mdï¼Œé¿å…é‡å¤
                report_md += "\n\n" + increment_md
                step_results.append({
                    **item,
                    "result_markdown": step_md,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "error": False
                })
                logger.info(f"[report] æ­¥éª¤ {step_index} å·²åˆå¹¶å®Œæˆ")

    # åˆ›å»ºä»»åŠ¡
    research_tasks = [
        asyncio.create_task(research_worker(i + 1, step))
        for i, step in enumerate(state["current_plan"].steps[:2])
    ]
    report_task = asyncio.create_task(report_worker())

    # ç­‰å¾…researchå®Œæˆ
    await asyncio.gather(*research_tasks)
    # å‘å‡ºç»“æŸä¿¡å·
    await report_queue.put(None)
    # ç­‰å¾…reportå®Œæˆ
    await report_task

    summary = await _async_add_summary_and_references(report_md)
    final_report = summary + f"\n\n---\nç ”ç©¶å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

    return Command(
        update={
            "messages": messages,
            "observations": state.get("observations", []) + all_research_messages,
            "research_summary": final_report,
            "step_results": step_results,
            "research_loop_count": state.get("research_loop_count", 0) + 1,
        },
        goto="__end__"
    )
      
# æ„å»ºå®Œæ•´çš„ç ”ç©¶æµç¨‹å›¾
graph_build = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph_build.add_node("coordinate", coordinate_node)
graph_build.add_node("generate_plan", generate_plan)
graph_build.add_node("human_feedback", human_back_node)
graph_build.add_node("research_node", async_research_node)

# å®šä¹‰æµç¨‹
graph_build.add_edge(START, "coordinate")

graph_build.add_edge("generate_plan", "human_feedback")
graph_build.add_edge("human_feedback", "research_node")
graph_build.add_edge("research_node", END)
graph_build.add_edge("coordinate", END)
# æ­£å¸¸å¯åŠ¨
from langgraph.checkpoint.memory import InMemorySaver
checkpointer = InMemorySaver() 
graph = graph_build.compile(checkpointer=checkpointer)

# é€šè¿‡ langgraph dev å¯åŠ¨ ï¼Œlanggraph API ä¼šè‡ªåŠ¨å¤„ç†æŒä¹…åŒ–ï¼Œä¸éœ€è¦è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ï¼Œå¦åˆ™æŠ¥é”™
# graph = graph_build.compile()
async def test_research_flow(research_topic: str = "ä½ æ˜¯è°", locale: str = "zh-CN"):
    """
    æµ‹è¯•å®Œæ•´çš„ç ”ç©¶æµç¨‹
    
    Args:
        research_topic: ç ”ç©¶ä¸»é¢˜
        locale: è¯­è¨€è®¾ç½®
    
    Returns:
        dict: åŒ…å«æµ‹è¯•ç»“æœå’Œæœ€ç»ˆçŠ¶æ€çš„å­—å…¸
    """
    print("=== ç ”ç©¶æµç¨‹æµ‹è¯•å¼€å§‹ ===")
    print(f"ç ”ç©¶ä¸»é¢˜: {research_topic}")
    print(f"è¯­è¨€è®¾ç½®: {locale}")
    
    config = {
        "configurable": {
            "thread_id": "test_research"
        }
    }
    
    
    # åˆ›å»ºæµ‹è¯•çŠ¶æ€
    test_state = State(
        messages=[HumanMessage(content=research_topic, name="ç”¨æˆ·è¾“å…¥")],
        research_topic=research_topic,
        locale=locale
    )
    
    results = {
        "research_topic": research_topic,
        "locale": locale,
        "stages": [],
        "final_state": None,
        "success": False,
        "error": None
    }
    
    try:
        # é˜¶æ®µ1: åè°ƒå™¨
        print("\nğŸ“‹ é˜¶æ®µ1: åè°ƒå™¨åˆ†æ...")
        async for result in graph.astream(test_state, config, stream_mode="messages"):
            print(result)
            print(type(result))

            print(result[0].content, end="", flush=True)
            stage_name = list(result)
            for i in range(len(result)):
                print(f"{i}: {result[i]}")
                print(f"{i}: {type(result[i])}")
            break
            results["stages"].append(stage_name)
            print(result)
            print(f"âœ… {stage_name}")   
        async for chunk in graph.astream(Command(resume={
                    "user_confirm": "confirm",
                    # "message": 
                }), config):
                print(chunk)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        results["error"] = str(e)
        import traceback
        traceback.print_exc()
        return results


# æµ‹è¯•è¿è¡Œ
if __name__ == "__main__":
    import asyncio
    
    result = asyncio.run(test_research_flow())
    