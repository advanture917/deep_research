from langgraph.types import Command ,interrupt 
from src.tools.search_with_image import TavilySearchWithImages
from src.llms.llm import get_llm
from langgraph.prebuilt import create_react_agent
from langgraph.graph import MessagesState,StateGraph , START , END
from typing import  List, Optional, Union
from typing_extensions import Annotated
import operator
from langchain_core.messages import HumanMessage
from langchain.tools import tool
from datetime import datetime
from src.prompts.template import render_prompt_template
from src.graph.type import Plan
import json
import logging
from langgraph.prebuilt import create_react_agent
from src.utils.content import ContextManager
logger = logging.getLogger(__name__)
# config = {"thread_id"}
class State(MessagesState):
    # ========== ç”¨æˆ·è¾“å…¥ ==========
    research_topic: str = ""                         # ç ”ç©¶ä¸»é¢˜ / ç”¨æˆ·é—®é¢˜
    locale: str = "zh-CN"                                # è¾“å‡ºè¯­è¨€ (en-US, zh-CN ç­‰)
    current_plan: Optional[Plan] = None                         # ç ”ç©¶è®¡åˆ’
    
    # ========== æœç´¢é˜¶æ®µ ==========
    search_queries: Annotated[List[str], operator.add] = []  # å·²ç”Ÿæˆçš„æŸ¥è¯¢
    web_results: Annotated[List[dict], operator.add] = []     # åŸå§‹æœç´¢ç»“æœ (title, url, snippet)
    sources: Annotated[List[dict], operator.add] = []         # è¿‡æ»¤/æç‚¼åçš„èµ„æ–™
    
    # ========== æ¨ç† & è®¡åˆ’ ==========
    observations: Annotated[List[str], operator.add] = []   # æ¯è½®ç ”ç©¶çš„å‘ç°/ç»“è®º
    plan_iterations: int = 0                                # è¿­ä»£æ¬¡æ•°
    auto_accept_plan: bool = False                          # æ˜¯å¦è‡ªåŠ¨æ¥å—ç”Ÿæˆçš„è®¡åˆ’
    
    # ========== èƒŒæ™¯è°ƒæŸ¥ ==========
    enable_background_investigation: bool = False        # æ˜¯å¦å¼€å¯å¹¶è¡ŒèƒŒæ™¯è°ƒæŸ¥
    background_info: Optional[str] = None                # èƒŒæ™¯ä¿¡æ¯ï¼ˆç»´åŸºç™¾ç§‘ç­‰ï¼‰
    
    # ========== ç ”ç©¶æ‰§è¡Œé˜¶æ®µ ==========
    research_summary: Optional[str] = None              # ç ”ç©¶æ€»ç»“æŠ¥å‘Š
    step_results: Annotated[List[dict], operator.add] = []     # æ¯ä¸ªç ”ç©¶æ­¥éª¤çš„è¯¦ç»†ç»“æœ
    
    # ========== æ§åˆ¶å‚æ•° ==========
    research_loop_count: int = 0                        # å½“å‰ç ”ç©¶å¾ªç¯æ•°
    max_research_loops: int = 3                                    # æœ€å¤§å¾ªç¯æ•°

llm = get_llm()
@tool 
def handoff_to_planner(
    research_topic: Annotated[str, "äº¤ç»™plannerå¤„ç†çš„ç ”ç©¶ä¸»é¢˜/ç”¨æˆ·é—®é¢˜"],
    locale: Annotated[str, "ç”¨æˆ·è¾“å…¥è¯­è¨€ (en-US, zh-CN ç­‰)"],
):
    """
    äº¤ç»™plannerå¤„ç†ç ”ç©¶ä¸»é¢˜
    """
    # è¿™ä¸ªå·¥å…·ä¸éœ€è¦è¿”å›å€¼ï¼Œåªè¦è¢«è°ƒç”¨å°±è¡¨ç¤ºéœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µ
    pass 

def coordinate_node(state: State) -> Command:
    """
    åè°ƒï¼Œå¤„ç†ç®€å•çš„ç”¨æˆ·æŸ¥è¯¢ï¼Œå¯¹äºç ”ç©¶ã€è®¡åˆ’ã€å¤æ‚çš„ä»»åŠ¡äº¤ç»™plannerå¤„ç†
    è½»é—®é¢˜ â†’ ç›´æ¥å›ç­”
    é‡é—®é¢˜ â†’ äº¤ç»™ planner
    """
    # è·å–ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜
    research_topic = state.get("research_topic", "")
    locale = state.get("locale", "zh-CN")
    
    # å¦‚æœæ²¡æœ‰ç ”ç©¶ä¸»é¢˜ï¼Œä½¿ç”¨ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºè¾“å…¥
    if not research_topic:
        # ä»æ¶ˆæ¯å†å²ä¸­è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        messages = state["messages"][-1]
        research_topic = messages.content

    # å‡†å¤‡åè°ƒå™¨æç¤ºæ¨¡æ¿
    template_vars = {
        "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        # "research_topic": research_topic,
    }
    # ä½¿ç”¨åè°ƒå™¨æç¤ºæ¨¡æ¿
    system_prompt = render_prompt_template("coordinate", **template_vars)
    llm_with_tools = llm.bind_tools([handoff_to_planner])
    
    # è¿è¡Œåè°ƒå™¨ä»£ç†
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": research_topic}
    ]
    
    result = llm_with_tools.invoke(messages)
    goto = "__end__"
    # æ£€æŸ¥å·¥å…·è°ƒç”¨
    tool_calls = []
    n = len(result.tool_calls)
    print(result)
    print(result.tool_calls)
    print(n)

    if n ==0 :
        print(f"goto: {goto}")
    # ç›´æ¥å›ç­”ç®€å•é—®é¢˜ï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯
        return Command(
            update={
                "messages": [{"role": "assistant", "content": result.content}]
            },
            goto= goto,
        )

    tool_calls.extend(result.tool_calls)
    goto = "generate_plan"    
    # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†handoff_to_plannerå·¥å…·
    handoff_calls = [call for call in tool_calls if call["name"] == "handoff_to_planner"]
    print(f"handoff_calls: {handoff_calls}")
    if handoff_calls:
        # æå–å·¥å…·è°ƒç”¨çš„å‚æ•°å¹¶å¡«å……åˆ°stateä¸­
        tool_call = handoff_calls[-1]  # ä½¿ç”¨æœ€åä¸€æ¬¡è°ƒç”¨çš„å‚æ•°
        args = tool_call["args"] if "args" in tool_call else {}

        research_topic = args.get("research_topic", research_topic)
        locale = args.get("locale", locale)
        
        # éœ€è¦è¿›å…¥ç ”ç©¶è®¡åˆ’é˜¶æ®µï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€
        return Command(
            update={
                "research_topic": research_topic,
                "locale": locale,
            },
            goto= goto,
            
        )
    


def generate_plan(state: State) -> dict:
    """
    ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼šç”Ÿæˆç ”ç©¶è®¡åˆ’
    ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿè·å–ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶é€šè¿‡Jinja2æ¨¡æ¿æ›¿æ¢å˜é‡
    """
    # å‡†å¤‡æ¨¡æ¿å˜é‡
    template_vars = {
        "CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "research_topic": state.get("research_topic", ""),
        "locale": state.get("locale", "zh-CN")
    }
    
    # ä½¿ç”¨å°è£…çš„render_prompt_templateæ–¹æ³•æ¸²æŸ“æ¨¡æ¿
    system_prompt = render_prompt_template("planner", **template_vars)
    
    # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{state.get('research_topic', '')}"}
    ]
    
    # è°ƒç”¨LLMç”Ÿæˆå“åº”
    struct_llm = llm.with_structured_output(Plan)
    response = struct_llm.invoke(messages)
    response_str = response.model_dump_json(indent=4)
    # è¿”å›å­—å…¸ç”¨äºæ›´æ–°çŠ¶æ€
    return {
        "current_plan": response,
        "plan_iterations": state.get("plan_iterations", 0) + 1,
        "messages": [{"role": "assistant", "content": response_str}]
    }

def human_back_node(state: State) -> Command:
    """
    ç”¨æˆ·æ˜¯å¦ç¡®è®¤planner ç”Ÿæˆçš„ç ”ç©¶è®¡åˆ’ï¼Œå¦‚æœç”¨æˆ·æœ‰è¡¥å……è¾“å…¥ï¼Œ
    åˆ™æ›´æ–°ç ”ç©¶è®¡åˆ’
    """

    feedback = interrupt("Please Review the Plan.")
    logger.info(f"ç”¨æˆ·åé¦ˆ: {feedback}")
    if feedback["user_confirm"] == "confirm":
        logger.info(f"ç”¨æˆ·ç¡®è®¤è®¡åˆ’ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶")
        goto = "research_node"
    else:
        goto = "generate_plan"
        logger.info(f"ç”¨æˆ·è¦æ±‚ä¿®æ”¹è®¡åˆ’ï¼Œé‡æ–°ç”Ÿæˆ")
        return Command(
            update={
            "messages": HumanMessage(content=feedback["message"],name = "feedback"),
            },
            goto = goto,
        )
    return Command(
        goto = goto,
    )

def research_node(state: State) -> Command:
    """
    åˆ†æ®µå¼ç ”ç©¶èŠ‚ç‚¹ï¼š
    æ¯ä¸ªæ­¥éª¤å®ŒæˆååŠ¨æ€ç”ŸæˆæŠ¥å‘Šæ®µè½ï¼Œ
    ä½¿ç”¨ ContextManager å‹ç¼©ä¸Šä¸‹æ–‡ã€‚
    """
    tools = [TavilySearchWithImages()]
    messages = state.get("messages", [])
    all_research_messages = []
    step_results = []
    research_summary_parts = []

    context_manager = ContextManager(llm, max_tokens=32768)
    messages = context_manager.compress(messages)

    for i, step in enumerate(state["current_plan"].steps[:2]):
        logger.info(f"æ‰§è¡Œç ”ç©¶æ­¥éª¤ {i+1}: {step.title}")

        # æ„å»ºæ­¥éª¤æç¤º
        step_prompt = f"""
        å½“å‰ç ”ç©¶æ­¥éª¤ {i+1}: {step.title}
        æè¿°: {step.description}

        è¯·æ ¹æ®å‰é¢çš„ç ”ç©¶å‘ç°ï¼ˆå¦‚æœ‰ï¼‰ç»§ç»­åˆ†æã€‚
        è¾“å‡ºï¼šé€»è¾‘æ¸…æ™°ã€ä¸“ä¸šåŒ–çš„ç ”ç©¶ç»“è®ºã€‚
        """

        # æ„å»ºæ¶ˆæ¯ä¸Šä¸‹æ–‡
        current_messages = messages + [{"role": "user", "content": step_prompt}]
        agent = create_react_agent(model=llm, tools=tools)

        try:
            # è°ƒç”¨ LLM agent
            result = agent.invoke({"messages": current_messages})
            ai_message = result["messages"][-1]
            raw_result = ai_message.content

            # ä¿å­˜åŸå§‹ç»“æœ
            step_results.append({
                "step_index": i,
                "title": step.title,
                "description": step.description,
                "result": raw_result,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })

            # è®© ContextManager ç”Ÿæˆè¯¥æ­¥éª¤çš„"æŠ¥å‘Šæ®µè½"
            from langchain_core.messages import SystemMessage, HumanMessage
            section_prompt = [
                SystemMessage(content="ä½ æ˜¯ç§‘ç ”æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚"),
                HumanMessage(content=f"è¯·å°†ä»¥ä¸‹ç ”ç©¶ç»“æœè½¬åŒ–ä¸ºç»“æ„åŒ–æŠ¥å‘Šæ®µè½ï¼Œé£æ ¼æ­£å¼ä¸”é€»è¾‘è¿è´¯ï¼š\n\n{raw_result}")
            ]
            section = llm.invoke(section_prompt).content
            research_summary_parts.append(section)

            # å‹ç¼©ä¸Šä¸‹æ–‡
            messages = context_manager.compress(
                messages + [
                    {"role": "user", "content": step_prompt},
                    {"role": "assistant", "content": raw_result}
                ]
            )

            all_research_messages.append(raw_result)
            logger.info(f"æ­¥éª¤ {i+1} å®Œæˆ")

        except Exception as e:
            logger.exception(f"æ‰§è¡Œæ­¥éª¤ {i+1} å‡ºé”™: {e}")
            step_results.append({
                "step_index": i,
                "title": step.title,
                "description": step.description,
                "result": f"ç ”ç©¶å¤±è´¥: {str(e)}",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "error": True
            })

    # æ‹¼æ¥æŠ¥å‘Š
    research_summary = f"# ç ”ç©¶æŠ¥å‘Š: {state['current_plan'].title}\n\n"
    research_summary += f"## èƒŒæ™¯ä¸ç ”ç©¶åŠ¨æœº\n{state['current_plan'].thought}\n\n"

    for idx, section in enumerate(research_summary_parts, start=1):
        research_summary += f"## {section}\n\n"

    research_summary += f"---\nç ”ç©¶å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

    return Command(
        update={
            "messages": messages,
            "observations": state.get("observations", []) + all_research_messages,
            "research_summary": research_summary,
            "step_results": step_results,
            "research_loop_count": state.get("research_loop_count", 0) + 1,
        },
        goto="__end__"
    )

        
# æ„å»ºå®Œæ•´çš„ç ”ç©¶æµç¨‹å›¾
graph_build = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph_build.add_node("coordinate", coordinate_node)
graph_build.add_node("generate_plan", generate_plan)
graph_build.add_node("human_feedback", human_back_node)
graph_build.add_node("research_node", research_node)

# å®šä¹‰æµç¨‹
graph_build.add_edge(START, "coordinate")

graph_build.add_edge("generate_plan", "human_feedback")
graph_build.add_edge("human_feedback", "research_node")
graph_build.add_edge("research_node", END)
graph_build.add_edge("coordinate", END)
# æ­£å¸¸å¯åŠ¨
from langgraph.checkpoint.memory import InMemorySaver
checkpointer = InMemorySaver() 
graph = graph_build.compile(checkpointer=checkpointer)

# é€šè¿‡ langgraph dev å¯åŠ¨ ï¼Œlanggraph API ä¼šè‡ªåŠ¨å¤„ç†æŒä¹…åŒ–ï¼Œä¸éœ€è¦è‡ªå®šä¹‰æ£€æŸ¥ç‚¹
# graph = graph_build.compile()
def test_research_flow(research_topic: str = "2025 å¹´ token2049å¤§ä¼šçš„å†…å®¹å’Œæ„¿æ™¯", locale: str = "zh-CN"):
    """
    æµ‹è¯•å®Œæ•´çš„ç ”ç©¶æµç¨‹
    
    Args:
        research_topic: ç ”ç©¶ä¸»é¢˜
        locale: è¯­è¨€è®¾ç½®
    
    Returns:
        dict: åŒ…å«æµ‹è¯•ç»“æœå’Œæœ€ç»ˆçŠ¶æ€çš„å­—å…¸
    """
    print("=== ç ”ç©¶æµç¨‹æµ‹è¯•å¼€å§‹ ===")
    print(f"ç ”ç©¶ä¸»é¢˜: {research_topic}")
    print(f"è¯­è¨€è®¾ç½®: {locale}")
    
    config = {
        "configurable": {
            "thread_id": "test_research"
        }
    }
    
    
    # åˆ›å»ºæµ‹è¯•çŠ¶æ€
    test_state = State(
        messages=[HumanMessage(content=research_topic, name="ç”¨æˆ·è¾“å…¥")],
        research_topic=research_topic,
        locale=locale
    )
    
    results = {
        "research_topic": research_topic,
        "locale": locale,
        "stages": [],
        "final_state": None,
        "success": False,
        "error": None
    }
    
    try:
        # é˜¶æ®µ1: åè°ƒå™¨
        print("\nğŸ“‹ é˜¶æ®µ1: åè°ƒå™¨åˆ†æ...")
        for result in graph.stream(test_state, config):
            stage_name = list(result.keys())[0]
            results["stages"].append(stage_name)
            print(f"âœ… {stage_name}")   
        for chunk in graph.stream(Command(resume={
                    "user_confirm": "confirm",
                    # "message": 
                }), config):
            print(chunk)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        results["error"] = str(e)
        import traceback
        traceback.print_exc()
        return results


# æµ‹è¯•è¿è¡Œ
if __name__ == "__main__":

    result = test_research_flow()
    